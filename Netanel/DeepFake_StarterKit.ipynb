{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFake Video Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DeepFake Starter Kit</h1>\n",
    "\n",
    "\n",
    "\n",
    "# <a id='0'>Content</a>\n",
    "\n",
    "- <a href='#1'>Introduction</a>  \n",
    "- <a href='#2'>Preliminary data exploration</a>  \n",
    "    * Load the packages  \n",
    "    * Load the data  \n",
    "    * Check files type  \n",
    "- <a href='#3'>Meta data exploration</a>  \n",
    "     * Missing data   \n",
    "     * Unique values  \n",
    "     * Most frequent originals  \n",
    "- <a href='#4'>Video data exploration</a>  \n",
    "     * Missing video (or meta) data  \n",
    "     * Few fake videos  \n",
    "     * Few real videos  \n",
    "     * Videos with same original  \n",
    "     * Test video files  \n",
    "     * Play video files\n",
    "- <a href='#5'>Face detection</a>  \n",
    "- <a href='#6'>Resources</a> \n",
    "- <a href='#7'>References</a>     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>Introduction</a>\n",
    "\n",
    "\n",
    "DeepFake is composed from Deep Learning and Fake and means taking one person  from an image or video and replacing with someone else\n",
    "likeness using technology such as Deep Artificial Neural Networks [1]. Large companies like Google invest very much in fighting the DeepFake, this including release of large datasets to help training models to counter this threat [2].The phenomen invades rapidly the film industry and threatens to compromise news agencies. Large digital companies, including content providers and social platforms are in the frontrun of fighting Deep Fakes. GANs that generate DeepFakes becomes better every day and, of course, if you include in a new GAN model all the information we collected until now how to combat various existent models, we create a model that cannot be beatten by the existing ones.   \n",
    "\n",
    "In the **Data Exploration** section we perform a (partial) Exploratory Data Analysis (EDA) on the training and testing data. After we are checking the files types, we are focusing first on the **metadata** files, which we are exploring in details, after we are importing in dataframes. Then, we move to explore video files, by looking first to a sample of fake videos, then to real videos. After that, we are also exploring few of the videos with the same origin. We are visualizing one frame extracted from the video, for both real and fake videos. Then we are also playing few videos.  \n",
    "Then, we move to perform face (and other `objects` from the persons in the videos) extraction. More precisely, we are using OpenCV Haar Cascade resources to identify frontal face, eyes, smile and profile face from still images in the videos.\n",
    "\n",
    "**Important note**: The data we analyze here is just a very small sample of data. The competition specifies that the train data is provided as archived chunks. Training of models should pe performed offline using the data provided by Kaggle as archives, models should be loaded (max 1GB memory) in a Kernel, where inference should be performed (submission sample file provided) and prediction should be prepared as an output file from the Kernel.\n",
    "\n",
    "\n",
    "In the **Resources** section I provide a short list of various resources for GAN and DeepFake, with blog posts, Kaggle Kernels and Github repos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>Preliminary data exploration</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets:\n",
    "There are 4 groups of datasets associated with this competition.\n",
    "\n",
    "1. **Training Set: This dataset, containing labels for the target, is available for download outside of Kaggle for competitors to build their models.** It is broken up into 50 files, for ease of access and download. Due to its large size, it must be accessed through a GCS bucket which is only made available to participants after accepting the competition’s rules. Please read the rules fully before accessing the dataset, as they contain important details about the dataset’s permitted use. It is expected and encouraged that you train your models outside of Kaggle’s notebooks environment and submit to Kaggle by uploading the trained model as an external data source.\n",
    "2. **Public Validation Set:** When you commit your Kaggle notebook, the submission file output that is generated will be based on the small set of 400 videos/ids contained within this Public Validation Set. This is available on the Kaggle Data page as test_videos.zip\n",
    "3. **Public Test Set: This dataset is completely withheld and is what Kaggle’s platform computes the public leaderboard against.** When you “Submit to Competition” from the “Output” file of a committed notebook that contains the competition’s dataset, your code will be re-run in the background against this Public Test Set. When the re-run is complete, the score will be posted to the public leaderboard. If the re-run fails, you will see an error reflected in your “My Submissions” page. Unfortunately, we are unable to surface any details about your error, so as to prevent error-probing. You are limited to 2 submissions per day, including submissions which error.\n",
    "4. **Private Test Set: This dataset is privately held outside of Kaggle’s platform, and is used to compute the private leaderboard.** It contains videos with a similar format and nature as the Training and Public Validation/Test Sets, but are real, organic videos with and without deepfakes. After the competition deadline, Kaggle transfers your 2 final selected submissions’ code to the host. They will re-run your code against this private dataset and return prediction submissions back to Kaggle for computing your final private leaderboard scores.\n",
    "### Workflow:\n",
    "This is a walk-through of how to make a submission to this code competition. The workflow is unique to this competition, so please read carefully.\n",
    "\n",
    "1. Accept the competition’s rules (Kaggle). Read these carefully before accepting, as there are important commitments you are making to the host about the use of their data.\n",
    "2. Access & download the Training Set (Google Cloud Storage Bucket). This is hosted outside of Kaggle. The link to this dataset is made available only after accepting the competition’s rules. See details under the “Datasets” section above.\n",
    "3. Train your models (Your choice - Local Machine, VM). You should be training your models outside of Kaggle using local or cloud resources. AWS is offering promotional credits for those needing additional computational resources to approach this challenge.\n",
    "4. Use Kaggle Notebooks (Notebook or Script) to create your submission (Kaggle). This is a code competition, so submissions must be made through Kaggle. You can upload any pre-trained or locally-trained models as data sources to your submission notebook. Your submission is subject to the Code Requirements mandated by this competition.\n",
    "5. Commit your Notebook/Script (Kaggle). Click commit on the notebook to run your code against the Public Validation Set. If successful, the notebook will generate a submission.csv file as an “Output”\n",
    "6. “Submit to Competition” from the “Output” file created (Kaggle). This will make your official submission to the competition. The “Submit to Competition” button will only be available if your notebook meets the requirements specified in Code Requirements. Your code will be re-run against the Public Test Set in the background. If successful, it will generate a score on the public leaderboard. You can review your submissions’ status on the “My Submissions” page. Note that this can take longer than the prior commit step due to the size of Public Test Set being larger than the Public Validation Set.\n",
    "7. Select 2 final submissions by the Submission Deadline (Kaggle). On your “My Submissions” page, your team leader can select the checkbox under “Use for final score” next to the 2 submissions to be re-run against the Private Test set.\n",
    "8. Wait and see! After the submission deadline, Kaggle will transfer your 2 selected submissions’ code to the host. They will re-run your code against their Private Test Set and return prediction submissions back to Kaggle for computing your final private leaderboard scores. For the private test set evaluation, the time constraint for your code’s run time will scale with that private test set's size. Review the Timeline Page for details of the expected timing for the private leaderboard review.\n",
    "### Tips for successful submissions:\n",
    "* Train your models outside of Kaggle. The training set is intentionally not available as a competition data source directly through Kaggle’s notebooks. It is not practical to train a model on Kaggle within the time constraints imposed.\n",
    "* Your code must output a submission.csv that predicts on any set of test_videos.zip that it is fed as an input. We’ve designed the Public Validation/Test Sets to aid in ensuring your code is robust enough for the Private Test Set. The Public Validation Set gives an example of what you can expect your model should handle while working in an interactive notebook session. The Public Test Set allows you to test your code’s performance on a withheld test set.\n",
    "* Ensure your code can be re-run on any unseen test set. Do not hard-code your submissions based on the public validation set. This will result in failure when your code runs against the public test set. You will not benefit from probing the Public Validation/Test Sets, because your code must perform honestly on the host’s fully Private Test Set.\n",
    "* You are limited to 9 hours of GPU compute time. This constraint is also imposed on the Public Test Set re-run. You should anticipate the Public Test set to be 10 times the size of the Public Validation Set and budget accordingly. The 9 hour compute constraint will be applied to the submission in Kaggle, when evaluated against the Public Test Set. So if you are able to make a successful submission that posts a score to the public leaderboard without an error, then it has met the 9 hour constraint on the Public Test Set. Review the full set of Code Requirements.\n",
    "* You are still limited to Kaggle’s global weekly per user GPU time constraints.\n",
    "* Don’t make the mistake of creating multiple accounts or private sharing. Any user who submits from multiple accounts will be removed at the end of the competition, without exception. This includes entire teams where only 1 member has created a multiple account. It also includes teams who are found to have shared privately with other teams or colluded with other teams to make any submissions. It also includes teams who engaged in private sharing prior to forming as a team. We will complete these removals at the end of the competition. If you have knowledge of any teams who have committed rules violations, please submit a claim to Kaggle Compliance with your specific evidence.\n",
    "* Resolving Submission Errors: Below is a quick rundown of the some errors that users may receive on submission. Note that we are not in a position to share more granular details of any precise exceptions/errors, as doing so would make the competition vulnerable to error-probing by savvy participants.\n",
    "  * Notebook Timeout: Your submission notebook exceeded the allowed runtime cap for the competition. Review the competition's Code Requirements for details, and note that the privately held rerun test set could be orders of magnitude larger than the publicly shared validation set.\n",
    "  * Submission Scoring Error: Your notebook generated a submission.csv file with incorrect format. Some examples causing this are having the wrong number of rows or columns, empty values, an incorrect datatype for a value, or invalid submission values from what is expected for the privately rerun test set.\n",
    "  * Notebook Threw Exception: While rerunning your code on the public test set (privately held), your notebook hit an error.\n",
    "Submission CSV Not Found: Your Notebook did not output a submission.csv file. Some examples causing this are if your Notebook times out, or if it runs into an error. There is a chance this will be the error returned instead of \"Notebook Timeout\" depending on error sequencing.\n",
    "  * No Submit to Competition button on my notebook's output: This indicates that you have enabled something that is prohibited for the submissions on that competition. Review the competition's Code Requirements for details\n",
    "  * Notebook Exceeded Allowed Compute: This indicates you have violated a Code Requirement constraint during the rerun.\n",
    "  * Kaggle Error: A rare system error. Please try resubmitting to resolve the error and contact Kaggle Support if it persists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "import os\n",
    "import matplotlib # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from tqdm import tqdm_notebook # type: ignore\n",
    "%matplotlib inline \n",
    "import cv2 as cv # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 401\n",
      "Test samples: 400\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = '../input/deepfake-detection'\n",
    "TRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n",
    "TEST_FOLDER = 'test_videos'\n",
    "\n",
    "print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\n",
    "print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added a face detection resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../input/haar-cascades-for-face-detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m FACE_DETECTION_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/haar-cascades-for-face-detection\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFace detection resources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFACE_DETECTION_FOLDER\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../input/haar-cascades-for-face-detection'"
     ]
    }
   ],
   "source": [
    "FACE_DETECTION_FOLDER = '../input/haar-cascades-for-face-detection'\n",
    "print(f\"Face detection resources: {os.listdir(FACE_DETECTION_FOLDER)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check files type\n",
    "\n",
    "Here we check the train data files extensions. Most of the files looks to have `mp4` extension, let's check if there is other extension as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))\n",
    "ext_dict = []\n",
    "for file in train_list:\n",
    "    file_ext = file.split('.')[1]\n",
    "    if (file_ext not in ext_dict):\n",
    "        ext_dict.append(file_ext)\n",
    "print(f\"Extensions: {ext_dict}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many files with each extensions there are.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_ext in ext_dict:\n",
    "    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same process for test videos folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))\n",
    "ext_dict = []\n",
    "for file in test_list:\n",
    "    file_ext = file.split('.')[1]\n",
    "    if (file_ext not in ext_dict):\n",
    "        ext_dict.append(file_ext)\n",
    "print(f\"Extensions: {ext_dict}\")\n",
    "for file_ext in ext_dict:\n",
    "    print(f\"Files with extension `{file_ext}`: {len([file for file in train_list if  file.endswith(file_ext)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the `json` file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = [file for file in train_list if  file.endswith('json')][0]\n",
    "print(f\"JSON file: {json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparently here is a metadata file. Let's explore this JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_from_json(path):\n",
    "    df = pd.read_json(os.path.join(DATA_FOLDER, path, json_file))\n",
    "    df = df.T\n",
    "    return df\n",
    "\n",
    "meta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)\n",
    "meta_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"3\">Meta data exploration</a>\n",
    "\n",
    "Let's explore now the meta data in train sample. \n",
    "\n",
    "## Missing data\n",
    "\n",
    "We start by checking for any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(data):\n",
    "    total = data.isnull().sum()\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
    "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in data.columns:\n",
    "        dtype = str(data[col].dtype)\n",
    "        types.append(dtype)\n",
    "    tt['Types'] = types\n",
    "    return(np.transpose(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(meta_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing data 19.25% of the samples (or 77). We suspect that actually the real data has missing original (if we generalize from the data we glimpsed). Let's check this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(meta_train_df.loc[meta_train_df.label=='REAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, all missing `original` data are the one associated with `REAL` label.  \n",
    "\n",
    "## Unique values;\n",
    "\n",
    "Let's check into more details the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_values(data):\n",
    "    total = data.count()\n",
    "    tt = pd.DataFrame(total)\n",
    "    tt.columns = ['Total']\n",
    "    uniques = []\n",
    "    for col in data.columns:\n",
    "        unique = data[col].nunique()\n",
    "        uniques.append(unique)\n",
    "    tt['Uniques'] = uniques\n",
    "    return(np.transpose(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values(meta_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that `original` label has the same pattern for uniques values. We know that we have 77 missing data (that's why total is only 323) and we observe that we do have 209 unique examples.  \n",
    "\n",
    "## Most frequent originals\n",
    "\n",
    "Let's look now to the most frequent originals uniques in train sample data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_values(data):\n",
    "    total = data.count()\n",
    "    tt = pd.DataFrame(total)\n",
    "    tt.columns = ['Total']\n",
    "    items = []\n",
    "    vals = []\n",
    "    for col in data.columns:\n",
    "        itm = data[col].value_counts().index[0]\n",
    "        val = data[col].value_counts().values[0]\n",
    "        items.append(itm)\n",
    "        vals.append(val)\n",
    "    tt['Most frequent item'] = items\n",
    "    tt['Frequence'] = vals\n",
    "    tt['Percent from total'] = np.round(vals / total * 100, 3)\n",
    "    return(np.transpose(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_values(meta_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most frequent **label** is `FAKE` (80.75%), `meawmsgiti.mp4` is the most frequent **original** (6 samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do now some data distribution visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(feature, title, df, size=1):\n",
    "    '''\n",
    "    Plot count of classes / feature\n",
    "    param: feature - the feature to analyze\n",
    "    param: title - title to add to the graph\n",
    "    param: df - dataframe from which we plot feature's classes distribution \n",
    "    param: size - default 1.\n",
    "    '''\n",
    "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
    "    total = float(len(df))\n",
    "    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n",
    "    g.set_title(\"Number and percentage of {}\".format(title))\n",
    "    if(size > 2):\n",
    "        plt.xticks(rotation=90, size=8)\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(100*height/total),\n",
    "                ha=\"center\") \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('split', 'split (train)', meta_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('label', 'label (train)', meta_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the `REAL` are only 19.25% in train sample videos, with the `FAKE`s acounting for 80.75% of the samples. \n",
    "\n",
    "\n",
    "# <a id=\"4\">Video data exploration</a>\n",
    "\n",
    "\n",
    "In the following we will explore some of the video data. \n",
    "\n",
    "\n",
    "## Missing video (or meta) data\n",
    "\n",
    "We check first if the list of files in the meta info and the list from the folder are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = np.array(list(meta_train_df.index))\n",
    "storage = np.array([file for file in train_list if  file.endswith('mp4')])\n",
    "print(f\"Metadata: {meta.shape[0]}, Folder: {storage.shape[0]}\")\n",
    "print(f\"Files in metadata and not in folder: {np.setdiff1d(meta,storage,assume_unique=False).shape[0]}\")\n",
    "print(f\"Files in folder and not in metadata: {np.setdiff1d(storage,meta,assume_unique=False).shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize now the data.  \n",
    "\n",
    "We select first a list of fake videos.\n",
    "\n",
    "## Few fake videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='FAKE'].sample(3).index)\n",
    "fake_train_sample_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [4] ([Basic EDA Face Detection, split video, ROI](https://www.kaggle.com/marcovasquez/basic-eda-face-detection-split-video-roi)) we modified a function for displaying a selected image from a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_video(video_path):\n",
    "    '''\n",
    "    input: video_path - path for video\n",
    "    process:\n",
    "    1. perform a video capture from the video\n",
    "    2. read the image\n",
    "    3. display the image\n",
    "    '''\n",
    "    capture_image = cv.VideoCapture(video_path) \n",
    "    ret, frame = capture_image.read()\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    ax.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in fake_train_sample_video:\n",
    "    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now the same for few of the images that are real.  \n",
    "\n",
    "\n",
    "## Few real videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_sample_video = list(meta_train_df.loc[meta_train_df.label=='REAL'].sample(3).index)\n",
    "real_train_sample_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in real_train_sample_video:\n",
    "    display_image_from_video(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER, video_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos with same original\n",
    "\n",
    "Let's look now to set of samples with the same original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_df['original'].value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick one of the originals with largest number of samples.   \n",
    "\n",
    "We also modify our visualization function to work with multiple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_from_video_list(video_path_list, video_folder=TRAIN_SAMPLE_FOLDER):\n",
    "    '''\n",
    "    input: video_path_list - path for video\n",
    "    process:\n",
    "    0. for each video in the video path list\n",
    "        1. perform a video capture from the video\n",
    "        2. read the image\n",
    "        3. display the image\n",
    "    '''\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2,3,figsize=(16,8))\n",
    "    # we only show images extracted from the first 6 videos\n",
    "    for i, video_file in enumerate(video_path_list[0:6]):\n",
    "        video_path = os.path.join(DATA_FOLDER, video_folder,video_file)\n",
    "        capture_image = cv.VideoCapture(video_path) \n",
    "        ret, frame = capture_image.read()\n",
    "        frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        ax[i//3, i%3].imshow(frame)\n",
    "        ax[i//3, i%3].set_title(f\"Video: {video_file}\")\n",
    "        ax[i//3, i%3].axis('on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='meawmsgiti.mp4'].index)\n",
    "display_image_from_video_list(same_original_fake_train_sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look now to a different selection of videos with the same original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='atvmxvwyns.mp4'].index)\n",
    "display_image_from_video_list(same_original_fake_train_sample_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='qeumxirsme.mp4'].index)\n",
    "display_image_from_video_list(same_original_fake_train_sample_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='kgbkktcjxf.mp4'].index)\n",
    "display_image_from_video_list(same_original_fake_train_sample_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test video files\n",
    "\n",
    "Let's also look to few of the test data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize now one of the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_video(os.path.join(DATA_FOLDER, TEST_FOLDER, test_videos.iloc[0].video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to some more videos from test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_from_video_list(test_videos.sample(6).video, TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='5'>Face detection</a>  \n",
    "\n",
    "From [5] ([Face Detection using OpenCV](https://www.kaggle.com/serkanpeldek/face-detection-with-opencv)) by [@serkanpeldek](https://www.kaggle.com/serkanpeldek) we got and slightly modified the functions to extract face, profile face, eyes and smile.  \n",
    "\n",
    "The class ObjectDetector initialize the cascade classifier (using the imported resource). The function **detect** uses a method of the CascadeClassifier to detect objects into images - in this case the face, eye, smile or profile face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector():\n",
    "    '''\n",
    "    Class for Object Detection\n",
    "    '''\n",
    "    def __init__(self,object_cascade_path):\n",
    "        '''\n",
    "        param: object_cascade_path - path for the *.xml defining the parameters for {face, eye, smile, profile}\n",
    "        detection algorithm\n",
    "        source of the haarcascade resource is: https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "        '''\n",
    "\n",
    "        self.objectCascade=cv.CascadeClassifier(object_cascade_path)\n",
    "\n",
    "\n",
    "    def detect(self, image, scale_factor=1.3,\n",
    "               min_neighbors=5,\n",
    "               min_size=(20,20)):\n",
    "        '''\n",
    "        Function return rectangle coordinates of object for given image\n",
    "        param: image - image to process\n",
    "        param: scale_factor - scale factor used for object detection\n",
    "        param: min_neighbors - minimum number of parameters considered during object detection\n",
    "        param: min_size - minimum size of bounding box for object detected\n",
    "        '''\n",
    "        rects=self.objectCascade.detectMultiScale(image,\n",
    "                                                scaleFactor=scale_factor,\n",
    "                                                minNeighbors=min_neighbors,\n",
    "                                                minSize=min_size)\n",
    "        return rects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the resources for frontal face, eye, smile and profile face detection.  \n",
    "\n",
    "Then we initialize the `ObjectDetector` objects defined above with the respective resources, to use CascadeClassfier for each specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frontal face, profile, eye and smile  haar cascade loaded\n",
    "frontal_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_frontalface_default.xml')\n",
    "eye_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_eye.xml')\n",
    "profile_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_profileface.xml')\n",
    "smile_cascade_path= os.path.join(FACE_DETECTION_FOLDER,'haarcascade_smile.xml')\n",
    "\n",
    "#Detector object created\n",
    "# frontal face\n",
    "fd=ObjectDetector(frontal_cascade_path)\n",
    "# eye\n",
    "ed=ObjectDetector(eye_cascade_path)\n",
    "# profile face\n",
    "pd=ObjectDetector(profile_cascade_path)\n",
    "# smile\n",
    "sd=ObjectDetector(smile_cascade_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function for detection and display of all these specific objects.  \n",
    "\n",
    "The function call the **detect** method of the **ObjectDetector** object. For each object we are using a different shape and color, as following:\n",
    "* Frontal face: green rectangle;  \n",
    "* Eye: red circle;  \n",
    "* Smile: red rectangle;  \n",
    "* Profile face: blue rectangle.  \n",
    "\n",
    "Note: due to a huge amount of false positive, we deactivate for now the smile detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image, scale_factor, min_neighbors, min_size):\n",
    "    '''\n",
    "    Objects detection function\n",
    "    Identify frontal face, eyes, smile and profile face and display the detected objects over the image\n",
    "    param: image - the image extracted from the video\n",
    "    param: scale_factor - scale factor parameter for `detect` function of ObjectDetector object\n",
    "    param: min_neighbors - min neighbors parameter for `detect` function of ObjectDetector object\n",
    "    param: min_size - minimum size parameter for f`detect` function of ObjectDetector object\n",
    "    '''\n",
    "    \n",
    "    image_gray=cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    eyes=ed.detect(image_gray,\n",
    "                   scale_factor=scale_factor,\n",
    "                   min_neighbors=min_neighbors,\n",
    "                   min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n",
    "\n",
    "    for x, y, w, h in eyes:\n",
    "        #detected eyes shown in color image\n",
    "        cv.circle(image,(int(x+w/2),int(y+h/2)),(int((w + h)/4)),(0, 0,255),3)\n",
    " \n",
    "    # deactivated due to many false positive\n",
    "    #smiles=sd.detect(image_gray,\n",
    "    #               scale_factor=scale_factor,\n",
    "    #               min_neighbors=min_neighbors,\n",
    "    #               min_size=(int(min_size[0]/2), int(min_size[1]/2)))\n",
    "\n",
    "    #for x, y, w, h in smiles:\n",
    "    #    #detected smiles shown in color image\n",
    "    #    cv.rectangle(image,(x,y),(x+w, y+h),(0, 0,255),3)\n",
    "\n",
    "\n",
    "    profiles=pd.detect(image_gray,\n",
    "                   scale_factor=scale_factor,\n",
    "                   min_neighbors=min_neighbors,\n",
    "                   min_size=min_size)\n",
    "\n",
    "    for x, y, w, h in profiles:\n",
    "        #detected profiles shown in color image\n",
    "        cv.rectangle(image,(x,y),(x+w, y+h),(255, 0,0),3)\n",
    "\n",
    "    faces=fd.detect(image_gray,\n",
    "                   scale_factor=scale_factor,\n",
    "                   min_neighbors=min_neighbors,\n",
    "                   min_size=min_size)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        #detected faces shown in color image\n",
    "        cv.rectangle(image,(x,y),(x+w, y+h),(0, 255,0),3)\n",
    "\n",
    "    # image\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    ax.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extracts an image from a video and then call the function that extracts the face rectangle from the image and display the rectangle above the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_objects(video_file, video_set_folder=TRAIN_SAMPLE_FOLDER):\n",
    "    '''\n",
    "    Extract one image from the video and then perform face/eyes/smile/profile detection on the image\n",
    "    param: video_file - the video from which to extract the image from which we extract the face\n",
    "    '''\n",
    "    video_path = os.path.join(DATA_FOLDER, video_set_folder,video_file)\n",
    "    capture_image = cv.VideoCapture(video_path) \n",
    "    ret, frame = capture_image.read()\n",
    "    #frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    detect_objects(image=frame, \n",
    "            scale_factor=1.3, \n",
    "            min_neighbors=5, \n",
    "            min_size=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the function for face detection for a selection of images from train sample videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_original_fake_train_sample_video = list(meta_train_df.loc[meta_train_df.original=='kgbkktcjxf.mp4'].index)\n",
    "for video_file in same_original_fake_train_sample_video[1:4]:\n",
    "    print(video_file)\n",
    "    extract_image_objects(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subsample_video = list(meta_train_df.sample(3).index)\n",
    "for video_file in train_subsample_video:\n",
    "    print(video_file)\n",
    "    extract_image_objects(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look to a small collection of samples from test videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_test_videos = list(test_videos.sample(3).video)\n",
    "for video_file in subsample_test_videos:\n",
    "    print(video_file)\n",
    "    extract_image_objects(video_file, TEST_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that in some cases, when the subject is not looking frontaly or when the luminosity is low, the algorithm for face detection is not detecting the face or eyes correctly. Due to a large amount of false positive, we deactivated for now the smile detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play video files  \n",
    "\n",
    "From [Play video and processing](https://www.kaggle.com/hamditarek/play-video-and-processing) Kernel by [@hamditarek](https://www.kaggle.com/hamditarek) we learned how to play video files in a Kaggle Kernel.  \n",
    "Let's look to few fake videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_videos = list(meta_train_df.loc[meta_train_df.label=='FAKE'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n",
    "    '''\n",
    "    Display video\n",
    "    param: video_file - the name of the video file to display\n",
    "    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n",
    "    '''\n",
    "    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
    "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_video(fake_videos[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From visual inspection of these fakes videos, in some cases is very easy to spot the anomalies created when engineering the deep fake, in some cases is more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"6\">Resources</a>  \n",
    "\n",
    "This resources list is not exhaustive, it provides just a starting point for Kagglers that would like to join this competition in order to learn, like myself.    \n",
    "\n",
    "I provide here technical articles links, small blog articles, links to Github projects and some inspirational Kaggle Kernels about DeepFake, DCGANs and GANs.   \n",
    "\n",
    "**Blogs, Technical Articles**\n",
    "\n",
    "* Towards Data Science DeepFakes subject selection: [DeepFakes](https://towardsdatascience.com/tagged/deepfakes)   \n",
    "\n",
    "* An introduction to DeepFakes from Towards Data Science: [Deepfakes: The Ugly, and The Good](https://towardsdatascience.com/deepfakes-the-ugly-and-the-good-49115643d8dd)  \n",
    "\n",
    "* Introduction to GAN from Towards Data Science (with code): [GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow](https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f)  \n",
    "\n",
    "* A cGAN introduction from Mastering Data Science (with code): [How to Develop a Conditional GAN (cGAN) From Scratch](https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/)   \n",
    "\n",
    "* An introduction to GANs on Analytics Vidhya,  [GANs — A Brief Introduction to Generative Adversarial Networks](https://medium.com/analytics-vidhya/gans-a-brief-introduction-to-generative-adversarial-networks-f06216c7200e?)\n",
    "\n",
    "**Tutorials**\n",
    "\n",
    "* A nice tutorial for GAN using Pytorch: [DCGAN Faces Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)  \n",
    "\n",
    "* A tutorial for GANs: [A Beginner's Guide to Generative Adversarial Networks (GANs)](https://pathmind.com/wiki/generative-adversarial-network-gan)  \n",
    "\n",
    "* Deep Convolutional Generative Adversial Networks, Tensorflow, [DCGAN](https://www.tensorflow.org/tutorials/generative/dcgan)    \n",
    "\n",
    "* A tutorial for video and image colorization and resolution improvement using fast.ai and PyTorch recommended by [@init27](https://www.kaggle.com/init27), [Decrappification, DeOldification, and Super Resolution](https://www.fast.ai/2019/05/03/decrappify/)  \n",
    "\n",
    "* A tutorial from OpenCV for face detection using Cascade Classifiers, [Cascade Classifier](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)  \n",
    "\n",
    "**Kaggle Kernels**\n",
    "\n",
    "* A very good intro to GAN, by [@nanashi](http://kaggle/nanashi): [GAN Introduction](https://www.kaggle.com/jesucristo/gan-introduction)    \n",
    "\n",
    "* A GAN developed for Dog face generation competition by [@cdeotte](http://kaggle/cdeotte): [Dog Memorizer GAN](https://www.kaggle.com/cdeotte/dog-memorizer-gan)    \n",
    "\n",
    "* A Kernel for Face detection using OpenCV with Haarcascade, by [@serkanpeldek](https://www.kaggle.com/serkanpeldek), [Face Detection with OpenCV](https://www.kaggle.com/serkanpeldek/face-detection-with-opencv)   \n",
    "\n",
    "* Play video and processing, by [@hamditarek](https://www.kaggle.com/hamditarek),  https://www.kaggle.com/hamditarek/play-video-and-processing   \n",
    "\n",
    "\n",
    "**Github repos**\n",
    "\n",
    "* Github topic for DeeFakes: [deepfakes](https://github.com/topics/)  \n",
    "\n",
    "* A Github project using Pytorch: [Faceswap-Deepfake-Pytorch](https://github.com/Oldpan/Faceswap-Deepfake-Pytorch)   \n",
    "\n",
    "* A Github project for GAN with PyTorch: [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)  \n",
    "\n",
    "* A Github recommended by [@shwetagoyal4](https://www.kaggle.com/shwetagoyal4), [Generative-model-using-PyTorch](https://github.com/Shwetago/Generative-model-using-PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"7\">References</a>\n",
    "\n",
    "[1] Deepfake, Wikipedia, https://en.wikipedia.org/wiki/Deepfake  \n",
    "[2] Google DeepFake Database, Endgadget, https://www.engadget.com/2019/09/25/google-deepfake-database/  \n",
    "[3] A quick look at the first frame of each video,  https://www.kaggle.com/brassmonkey381/a-quick-look-at-the-first-frame-of-each-video  \n",
    "[4] Basic EDA Face Detection, split video, ROI, https://www.kaggle.com/marcovasquez/basic-eda-face-detection-split-video-roi  \n",
    "[5] Face Detection with OpenCV, https://www.kaggle.com/serkanpeldek/face-detection-with-opencv   \n",
    "[6] Play video and processing, https://www.kaggle.com/hamditarek/play-video-and-processing/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
