{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional\n",
    "import pyopencl as cl\n",
    "import tensorflow as tf \n",
    "from rocm.configure import * \n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import keras\n",
    "import getpass\n",
    "from imutils import paths\n",
    "import imageio\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./configs/data_path.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "USER = getpass.getuser()\n",
    "\n",
    "# Large Dataset\n",
    "DATA_FOLDER = data['dataset'][USER][0]\n",
    "COMPRESSED_DATA_FOLDER = data['dataset'][USER][1]\n",
    "DATA_DIRECTORIES = os.listdir(DATA_FOLDER)\n",
    "\n",
    "# Define the data directories\n",
    "TEST_FOLDER = data['test']\n",
    "TRAIN_FOLDER = data['train']\n",
    "VAL_FOLDER = data['val']\n",
    "\n",
    "# Define hyperparameters\n",
    "IMG_SIZE = data['image_size']\n",
    "BATCH_SIZE = data['batch_size']\n",
    "EPOCHS = data['epochs']\n",
    "MAX_SEQ_LENGTH = data['max_seq_length']\n",
    "NUM_FEATURES = data['num_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function read video metadata from json files\n",
    "def read_meta_from_json(data_directories, data_folder, processed_data_folder):\n",
    "    \n",
    "    meta_df = pd.DataFrame()\n",
    "\n",
    "    for index, part_folder in enumerate(data_directories):\n",
    "        part_path = os.path.join(data_folder, part_folder)\n",
    "        json_file = next(file for file in os.listdir(part_path) if file.endswith('json'))\n",
    "        json_file_path = os.path.join(part_path, json_file)\n",
    "        \n",
    "        part_df = pd.read_json(json_file_path).T\n",
    "        part_df['part'] = index\n",
    "        part_df['path'] = part_path\n",
    "        part_df['path-compressed'] =  os.path.join(processed_data_folder, os.path.basename(part_path))\n",
    "        part_df['filename'] = part_df.index\n",
    "        \n",
    "        meta_df = pd.concat([meta_df, part_df])\n",
    "\n",
    "    # Display 5 random rows from meta_df\n",
    "    meta_df.reset_index(drop=True, inplace=True)\n",
    "    display(meta_df.head(n=5))\n",
    "    return meta_df    \n",
    "\n",
    "# Function to check data types\n",
    "def explore_files(files):\n",
    "    ext_dict = []\n",
    "    for file in files:\n",
    "        file_ext = file.split('.')[1]\n",
    "        if (file_ext not in ext_dict):\n",
    "            ext_dict.append(file_ext) \n",
    "\n",
    "    for file_ext in ext_dict:\n",
    "        print(f\"Files with extension `{file_ext}`: {len([file for file in files if  file.endswith(file_ext)])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 0: Define the Device (GPU)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tf.config.list_physical_devices('GPU')\n",
    "print(f'Using device:\\n{device[0][0]} for training with tensorflow {tf.__version__}')\n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(f\"Num CPUs Available: {len(tf.config.list_physical_devices('CPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set GPU Memory Growth**\n",
    "By default, TensorFlow allocates all GPU memory when it starts. If you want TensorFlow to allocate memory only as needed, you can set GPU memory growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpus:= tf.config.experimental.list_physical_devices('GPU'):\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Allow TensorFlow to see only one GPU\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models  # type: ignore\n",
    "\n",
    "def build_3d_cnn_model(input_shape):\n",
    "    return models.Sequential()\n",
    "\n",
    "model = build_3d_cnn_model((32, 32, 32, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Define the Dataset and DataLoader**\n",
    "\n",
    "The data is comprised of .mp4 files, split into compressed sets of ~10GB apiece. A metadata.json accompanies each set of .mp4 files, and contains filename, label (REAL/FAKE), original and split columns, listed here:\n",
    "- ```filename```  -  the filename of the video\n",
    "- ```label```  -  whether the video is REAL or FAKE\n",
    "- ```original```  -  in the case that a train set video is FAKE, the original video is listed here\n",
    "- ```split```  -  this is always equal to \"train\".\n",
    "\n",
    "#### What are we predicting?\n",
    "We predict whether or not a particular video is a deepfake. A deepfake could be **either** a face or voice swap (or both). In the training data, this is denoted by the string ```\"REAL\"``` or ```\"FAKE\"``` in the label column. In your submission, you will predict the probability that the video is a fake. The full training set is just over 470 GB, divided into 50 smaller files, each ~10 GB in size. We decided to split this data according to the following ratio:\n",
    "\n",
    "**Training Set** - ``80%`` | **Validation Set** - ``10%`` | **Test Set** - ``10%``\n",
    "\n",
    "\n",
    "1. Read `metadata.json`files from each folder\n",
    "2. Load the data\n",
    "3. Split the data (train, validation, and test)\n",
    "4. Process and save the data\n",
    "5. Load the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all json metadata files into a dataframe\n",
    "meta_df = read_meta_from_json(DATA_DIRECTORIES, DATA_FOLDER, COMPRESSED_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path):\n",
    "    video = tf.io.read_file(video_path)\n",
    "    video = tf.io.decode_video(video)\n",
    "    video = tf.image.resize(video, (224, 224), method='bilinear', preserve_aspect_ratio=True)\n",
    "    video = tf.cast(video, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return video\n",
    "\n",
    "def load_video_dataset(file_paths, labels):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: (preprocess_video(x), y))\n",
    "    dataset = dataset.batch(8)\n",
    "    return dataset\n",
    "\n",
    "print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_FOLDER)))}\")\n",
    "print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")\n",
    "\n",
    "# Example file paths and labels\n",
    "# train_file_paths = [\"path/to/video1.mp4\", \"path/to/video2.mp4\", ...]\n",
    "# train_labels = [0, 1, ...]  # Corresponding labels\n",
    "\n",
    "# train_dataset = load_video_dataset(train_file_paths, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Define the Loss Function and Metrics (Score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    return tn / (tn + fp + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
    "\n",
    "def matthews_correlation_coefficient(y_true, y_pred):\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "\n",
    "    num = tp * tn - fp * fn\n",
    "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
    "    return num / K.sqrt(den + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` and `train_dataset` are defined as before\n",
    "model.compile(optimizer='adam', loss='binary_focal_crossentropy', metrics=['accuracy', recall, precision, specificity, f1, matthews_correlation_coefficient])\n",
    "\n",
    "# Train the model on GPU\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Plot Training Process**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
